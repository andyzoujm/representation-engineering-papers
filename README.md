# Representation Engineering Papers

| Author(s) | Title |
|:--------- | ----- |
| Hendel et al. | [In-Context Learning Creates Task Vectors](https://arxiv.org/abs/2310.15916) |
| Liu et al. | [In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering](https://arxiv.org/abs/2311.06668) |
| Schut et al. | [Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero](https://arxiv.org/abs/2310.16410) |
| Todd et al. | [Function Vectors in Large Language Models](https://arxiv.org/abs/2310.15213) |
| Gandelsman et al. | [Interpreting CLIP's Image Representation via Text-Based Decomposition](https://arxiv.org/abs/2310.05916) |
| Marks et al. | [The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets](https://arxiv.org/abs/2310.06824) |
| Park et al. | [The Linear Representation Hypothesis and the Geometry of Large Language Models](https://arxiv.org/abs/2311.03658) |
| Nylund et al. | [Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401) |
| Karvonen | [Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models](https://arxiv.org/abs/2403.15498) |
| Wu et al. | [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592) |
| Hojel et al. | [Finding Visual Task Vectors](https://arxiv.org/abs/2404.05729) |
